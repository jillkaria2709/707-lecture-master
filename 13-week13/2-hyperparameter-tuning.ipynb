{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Keras Tuner\n",
    "\n",
    "Keras Tuner is a powerful tool for hyperparameter tuning in Keras models. Hyperparameter tuning is the process of searching for the optimal set of hyperparameters (parameters that govern the training process) for a machine learning model. This process is crucial because the choice of hyperparameters can significantly impact the performance of a model.\n",
    "\n",
    "#### What Can Be Tuned?\n",
    "\n",
    "With Keras Tuner, you can tune virtually any aspect of a model. Common hyperparameters include:\n",
    "\n",
    "1. **Learning Rate:** One of the most critical hyperparameters for training neural networks.\n",
    "2. **Number of Layers:** The depth of the network.\n",
    "3. **Number of Neurons in Each Layer:** Influences the model's capacity.\n",
    "4. **Activation Functions:** Such as ReLU, sigmoid, or tanh.\n",
    "5. **Batch Size:** Number of samples processed before the model is updated.\n",
    "6. **Optimizer:** Such as Adam, SGD, etc.\n",
    "7. **Dropout Rate:** Used in dropout layers to prevent overfitting.\n",
    "\n",
    "#### Types of Tuners Available\n",
    "\n",
    "Keras Tuner offers several algorithms for hyperparameter optimization:\n",
    "\n",
    "1. **Random Search:** Randomly selects a combination of hyperparameters to construct a model. This method is simple and can be surprisingly effective.\n",
    "\n",
    "2. **Hyperband:** An optimized version of random search which uses a bandit-based approach to allocate resources. It's more efficient as it early-stops underperforming trials.\n",
    "\n",
    "3. **Bayesian Optimization:** Uses probability to model the function and then makes decisions on where to sample next. It’s more systematic compared to random search and often yields better results.\n",
    "\n",
    "4. **Sklearn:** If you are using Keras models with the Scikit-learn API, this tuner adapts the hyperparameter search to this interface.\n",
    "\n",
    "### Example: Hyperparameter Tuning with Fashion MNIST\n",
    "\n",
    "#### Fashion MNIST Dataset\n",
    "\n",
    "We'll demo the hyperparameter tuning using the Fashion MNIST data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras_tuner) (3.2.1)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from keras_tuner) (24.0)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from keras_tuner) (2.31.0)\n",
      "Collecting kt-legacy (from keras_tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: absl-py in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras->keras_tuner) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from keras->keras_tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras->keras_tuner) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras->keras_tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras->keras_tuner) (3.11.0)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras->keras_tuner) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras->keras_tuner) (0.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->keras_tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->keras_tuner) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->keras_tuner) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->keras_tuner) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from optree->keras->keras_tuner) (4.10.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich->keras->keras_tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich->keras->keras_tuner) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras_tuner\n",
      "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# If necessary, install keras tuner\n",
    "%pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 13:43:33.407900: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-18 13:43:33.413614: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-18 13:43:33.463209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 13:43:34.623429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Load the data\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "\n",
    "#Clear the session for a fresh start\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    # Set up parameters in the \"hp\" object, which is passed in by the keras tuner when iterating through\n",
    "    # different models\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
    "\n",
    "    # Log sampling samples evenly across a log scale variable\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
    "                             sampling=\"log\")\n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # The following builds the model based on the parameters from the hp model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to do a search, you initialize the search object by passing in the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 29s]\n",
      "val_accuracy: 0.8367999792098999\n",
      "\n",
      "Best val_accuracy So Far: 0.8622000217437744\n",
      "Total elapsed time: 00h 02m 37s\n"
     ]
    }
   ],
   "source": [
    "# Note that max trials is 5=, indicating only 5 tests will be made\n",
    "random_search_tuner = kt.RandomSearch(build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True, directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\n",
    "\n",
    "# Finally, this kicks it all off\n",
    "random_search_tuner.search(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each trial, the tuner builds a model using hyperparameters sampled randomly within their respective ranges, then trains that model for 10 epochs and saves it to a subdirectory of the `my_fashion_mnist/my_rnd_search` directory. Since `overwrite=True`, the my_rnd_search directory is deleted before training starts. If you run this code a second time but with `overwrite=False` and `max_tri⁠als=10`, the tuner will continue tuning where it left off, running 5 more trials: this means you don’t have to run all the trials in one shot. Lastly, since objective is set to `val_accuracy`, the tuner prefers models with a higher validation accuracy, so once the tuner has finished searching, you can get the best models like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_models = random_search_tuner.get_best_models(num_models=3) \n",
    "best_model = top3_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can get the best hyperparameters directly like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_hidden': 8,\n",
       " 'n_neurons': 37,\n",
       " 'learning_rate': 0.008547485565344062,\n",
       " 'optimizer': 'sgd'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n",
    "top3_params[0].values # best hyperparameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get a richer summary by querying the \"oracle\" associated with a tuner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 8\n",
      "n_neurons: 37\n",
      "learning_rate: 0.008547485565344062\n",
      "optimizer: sgd\n",
      "Score: 0.8622000217437744\n"
     ]
    }
   ],
   "source": [
    "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_trial.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get your best performing model and continue training with the full set if you'd like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8654 - loss: 0.3718\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3612\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 988us/step - accuracy: 0.8704 - loss: 0.3532\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8739 - loss: 0.3470\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8758 - loss: 0.3411\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8769 - loss: 0.3371\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8789 - loss: 0.3313\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8796 - loss: 0.3242\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8799 - loss: 0.3245\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8821 - loss: 0.3188\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.8519 - loss: 0.4316\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model.fit(X_train_full, y_train_full, epochs=10) \n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning preprocessing\n",
    "\n",
    "It is also possible to use the Keras Tuner to explore different pre-processing strategies.  However, to do this, it is necessary to extend the HyperModel class and define the `build()` and `fit()` methods.  The `fit()` method takes a hyperparameters object and a compiled model, as well as all other arguments to fit, and returns a history object.  The `fit()` methods determines how to preprocess data, tweak the batch size, etc.  For instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyClassificationHyperModel(kt.HyperModel): \n",
    "    def build(self, hp):\n",
    "        # Just using the build_model function we defined previously\n",
    "        return build_model(hp)\n",
    "    \n",
    "    def fit(self, hp, model, X, y, **kwargs): \n",
    "        # Here, we decide whether or not to include a normalilzation layer based on a hyperparameter setting.\n",
    "        if hp.Boolean(\"normalize\"):\n",
    "            # Note, the Normalization \"layer\" here is just being use to normalize the data.  It is not actually added to the network.\n",
    "            # This means that it is only applied to training data!  We'd probably want it as part of the model in a production scenario.\n",
    "            norm_layer = tf.keras.layers.Normalization()\n",
    "            X = norm_layer(X)\n",
    "        return model.fit(X, y, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass this to a tuner of our choice.  We'll use the hyperband tuner, which works by training several models for a few epochs, and then discards the worst models and continues with the top 1/factor (a parameter) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperband_tuner = kt.Hyperband(\n",
    "MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42, max_epochs=10, factor=3, hyperband_iterations=2,\n",
    "overwrite=True, directory=\"my_fashion_mnist\", project_name=\"hyperband\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use TensorBoard here to analyze our results for each of the different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 43s]\n",
      "val_accuracy: 0.8442000150680542\n",
      "\n",
      "Best val_accuracy So Far: 0.8695999979972839\n",
      "Total elapsed time: 00h 18m 46s\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\" \n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir) \n",
    "#Also using an early stopping callback here\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2) \n",
    "# Finally, search\n",
    "hyperband_tuner.search(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tensorboard..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost:6006/\">http://localhost:6006/</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir=my_fashion_mnist/hyperband/tensorboard \n",
    "from IPython.display import display, HTML\n",
    "# Display this inline here\n",
    "display(HTML('<a href=\"http://localhost:6006/\">http://localhost:6006/</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide on Selecting Hyperparameters\n",
    "\n",
    "Selecting the right hyperparameters is crucial for training effective neural network models. Here's a quick guide covering key hyperparameters, along with rules of thumb and intuitions for training:\n",
    "\n",
    "#### 1. Number of Hidden Layers\n",
    "\n",
    "- **General Rule:** Start with one or two hidden layers for simple problems. For more complex problems, gradually increase the number of layers.\n",
    "- **Deep Networks:** More layers allow the network to learn more complex patterns (hierarchical feature learning). However, deeper networks are harder to train and more prone to overfitting.\n",
    "- **Transfer Learning:** Re-using hidden layers from a pre-trained model can significantly boost performance, especially when data is limited. This is effective when the new task is similar to the task originally trained on.\n",
    "\n",
    "#### 2. Number of Neurons per Hidden Layer\n",
    "\n",
    "- **Size Configurations:** Often configured in a funnel shape (e.g., decreasing number of neurons in each successive layer).\n",
    "- **Capacity of the Model:** More neurons increase the learning capacity but can lead to overfitting and longer training times.\n",
    "- **Rule of Thumb:** A common practice is to use a number of neurons that is between the number of input and output neurons, experimenting with values to find the optimal size.\n",
    "\n",
    "#### 3. Learning Rate\n",
    "\n",
    "- **Impact:** One of the most critical hyperparameters. Too low, and training will be slow; too high, and the network may not converge.\n",
    "- **Adaptive Learning Rates:** Techniques like learning rate annealing (gradually reducing the learning rate) can be very effective.\n",
    "- **Optimization Techniques:** Upcoming topics like learning rate schedules and adaptive learning rate methods (e.g., Adam, RMSprop) offer more nuanced control.\n",
    "\n",
    "#### 4. Optimizer\n",
    "\n",
    "- **Role of Optimizer:** Determines how the network will be updated based on the loss gradient. It affects the speed and quality of the learning process.\n",
    "- **Choices:** Common optimizers include SGD (Stochastic Gradient Descent), Adam, RMSprop. Adam is a good default choice, as it combines the benefits of other extensions of SGD.\n",
    "\n",
    "#### 5. Batch Size\n",
    "\n",
    "- **Trade-offs:** Larger batches provide more accurate estimates of the gradient, but smaller batches offer a regularizing effect and less stable, noisier updates, which can help escape local minima.\n",
    "- **Hardware Constraints:** Limited by memory constraints. Larger batches require more memory.\n",
    "- **Typical Values:** Common batch sizes range from 32 to 256, but the optimal size depends on the specific problem and hardware.\n",
    "\n",
    "#### 6. Activation Function\n",
    "\n",
    "- **Non-linearity:** Functions like ReLU (and its variants like Leaky ReLU) are common because they help with faster training and reduce the likelihood of vanishing gradients in deep networks.\n",
    "- **Problem Specific:** For the output layer, use softmax for multi-class classification and sigmoid for binary classification.\n",
    "\n",
    "#### 7. Number of Iterations and Early Stopping\n",
    "\n",
    "- **Epochs vs. Iterations:** An epoch is a complete pass over the entire training dataset, while an iteration is a single update of the model's parameters.\n",
    "- **Early Stopping:** A technique to prevent overfitting by stopping training when the model's performance on a validation set starts to degrade.\n",
    "- **Monitoring Performance:** Adjust the number of epochs based on monitoring performance metrics on a validation set.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Selecting hyperparameters is often more art than science, requiring experimentation and iteration. It's common to start with certain defaults and adjust based on the specific problem and observed performance. Transfer learning, adaptive learning rates, and early stopping are advanced strategies that can significantly improve model performance and training efficiency. Remember, the goal is to find a balance that allows the network to learn effectively without overfitting, underfitting, or requiring excessive computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
